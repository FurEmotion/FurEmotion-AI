{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 음성 데이터 전처리\n",
    "\n",
    "전체 반려동물 울음소리 음성 데이터(.wav)에 대해 전처리를 수행한다.\n",
    "\n",
    "음성의 진폭과 주파수에 따른 특징이 잘 드러나도록 세그먼트 분할, 노이즈 제거, 등의 작업을 1~6 번 과정에서 수행하며\n",
    "\n",
    "7~9 번 과정에서 음성 길이 통일, masking, 등의 과정을 통해 동일한 크기의 벡터 생성 및 데이터 증강을 수행한다.\n",
    "\n",
    "본 문서는 수집된 전체 음성 데이터에 대해 수행되며 각 단계에 대한 필요성 언급 및 시각화를 하지 않는다. 이는 전체 데이터를 효과적으로 전처리하기 위함이며 각 단계의 필요성 및 시각적 자료는 trans_data_preview.ipynb 파일에 명시되어 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "전처리 순서는 아래와 같다.\n",
    "\n",
    "1. sample rate를 16000으로 통일한다.\n",
    "\n",
    "2. 로그 멜 스펙트럼을 통해 음성의 파워를 측정한다.\n",
    "\n",
    "3. 파워의 지역 최솟값을 기준으로 음성을 분할한다.\n",
    "\n",
    "4. 분할된 음성의 앞뒤 화이트 노이즈(특정 임계값 이하의 에너지를 가지는 시점)을 제거한다.\n",
    "\n",
    "5. 분할된 음성이 노이즈를 제거하여 패턴을 보다 잘 드러나게 한다.\n",
    "\n",
    "6. 음성 정규화를 수행한다.\n",
    "\n",
    "7. Padding과 Trimming을 통해 평균 세그멘테이션 길이를 가지도록 음성의 길이를 통일한다.\n",
    "\n",
    "8. 음성의 유사도를 측정하여 하위 10%의 데이터는 제외한다.\n",
    "\n",
    "9. 유사도 상위 10% 데이터에 대하여 SpecAugment를 적용하여 Training 데이터에 대해 마스킹 작업을 수행한다. 각 파일 당 마스킹 파일 2개를 생성한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import sys\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_animal = 'dog'  # 'cat', 'dog'\n",
    "\n",
    "# Set Path\n",
    "main_path = os.path.join(os.getcwd().rsplit(\n",
    "    'FurEmotion-AI')[0], 'FurEmotion-AI')\n",
    "data_path = os.path.join(main_path, 'data', target_animal)\n",
    "temp_data_path = os.path.join(main_path, 'temp_data', target_animal)\n",
    "csv_path = os.path.join(main_path, 'origin_data_info.csv')\n",
    "origin_data_path = os.path.join(main_path, 'origin_data', target_animal)\n",
    "\n",
    "sys.path.append(main_path)\n",
    "\n",
    "if not os.path.exists(temp_data_path):\n",
    "    os.makedirs(temp_data_path)\n",
    "if not os.path.exists(data_path):\n",
    "    os.makedirs(data_path)\n",
    "if not os.path.exists(origin_data_path):\n",
    "    raise ValueError(f'No such animal data path: {origin_data_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state list:  ['relax', 'hostile', 'whining']\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터\n",
    "search_in_sec = 3   # 파워값을 측정하는 시간 간격\n",
    "\n",
    "state_list = [dir_path for dir_path in os.listdir(origin_data_path) if os.path.isdir(\n",
    "    os.path.join(origin_data_path, dir_path))]\n",
    "\n",
    "# play는 데이터양이 너무 적어서 제외한다.\n",
    "if 'play' in state_list:\n",
    "    state_list.remove('play')\n",
    "print(\"state list: \", state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수들을 정의한다.\n",
    "def show_spectrum(y, sr):\n",
    "    # STFT 계산\n",
    "    D = librosa.amplitude_to_db(abs(librosa.stft(y)), ref=np.max)\n",
    "\n",
    "    # 시간과 주파수 축을 위한 값들 계산\n",
    "    times = np.linspace(0, len(y) / sr, num=D.shape[1], endpoint=False)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=2048)\n",
    "\n",
    "    # 스펙트럼 시각화\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(D, aspect='auto', origin='lower', extent=[\n",
    "               times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_mel_power(power, dot_list=[], dot_color='red', dot_label=''):\n",
    "    # 파워 시각화\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(power)\n",
    "    plt.title(\"Log Mel Spectrum Power\")\n",
    "    plt.xlabel(\"Time Frame\")\n",
    "    plt.ylabel(\"Power\")\n",
    "\n",
    "    if (len(dot_list) > 0):\n",
    "        plt.scatter(dot_list, power[dot_list],\n",
    "                    color=dot_color, label=dot_label)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_masked_mel(db_masked_mel):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(db_masked_mel, origin='lower', aspect='auto',\n",
    "               extent=[0, db_masked_mel.shape[1], 0, db_masked_mel.shape[0]])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Realistically Masked Mel spectrogram (using matplotlib)')\n",
    "    plt.xlabel('Time frames')\n",
    "    plt.ylabel('Mel frequency bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relax: 71 ['/Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/relax/Dog190.wav', '/Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/relax/Dog184.wav']\n",
      "hostile: 69 ['/Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/hostile/dog59.wav', '/Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/hostile/dog65.wav']\n",
      "whining: 181 ['/Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining122.wav', '/Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining136.wav']\n"
     ]
    }
   ],
   "source": [
    "# 데이터 경로를 불러오는 함수를 정의한다.\n",
    "def get_sound_files_by_state(target_path, state_list):\n",
    "    sound_files = {}\n",
    "    for state in state_list:\n",
    "        sound_files[state] = []\n",
    "\n",
    "    for (root, dir, files) in os.walk(target_path):\n",
    "        folder_state = root.split('/')[-1]\n",
    "        if folder_state not in state_list:\n",
    "            continue\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                sound_files[root.split(\n",
    "                    '/')[-1]].append(os.path.join(root, file))\n",
    "    return sound_files\n",
    "\n",
    "\n",
    "origin_sound_files = get_sound_files_by_state(origin_data_path, state_list)\n",
    "for state in state_list:\n",
    "    print(\n",
    "        f'{state}: {len(origin_sound_files[state])}', origin_sound_files[state][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:00<00:00, 86.42it/s]\n",
      "100%|██████████| 69/69 [00:00<00:00, 82.69it/s]\n",
      " 43%|████▎     | 78/181 [00:00<00:00, 108.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining182.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining183.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining181.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining180.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 102/181 [00:00<00:00, 110.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining184.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining178.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining179.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining174.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 126/181 [00:01<00:00, 111.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining175.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining177.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining176.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining172.wav\n",
      "Error in /Users/jaewone/Downloads/FurEmotion-AI/origin_data/dog/whining/whining173.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 181/181 [00:01<00:00, 106.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relax: 71 ['/Users/jaewone/Downloads/FurEmotion-AI/temp_data/dog/relax/Dog190.wav', '/Users/jaewone/Downloads/FurEmotion-AI/temp_data/dog/relax/Dog184.wav']\n",
      "hostile: 69 ['/Users/jaewone/Downloads/FurEmotion-AI/temp_data/dog/hostile/dog59.wav', '/Users/jaewone/Downloads/FurEmotion-AI/temp_data/dog/hostile/dog65.wav']\n",
      "whining: 168 ['/Users/jaewone/Downloads/FurEmotion-AI/temp_data/dog/whining/whining122.wav', '/Users/jaewone/Downloads/FurEmotion-AI/temp_data/dog/whining/whining136.wav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. sampling reate를 16000으로 통일한다.\n",
    "import sox\n",
    "\n",
    "\n",
    "def resampling(file_path_list: list[str], output_path: str, target_sample_rate: int = 16000):\n",
    "    \"\"\"\n",
    "    Change sample rate\n",
    "\n",
    "    Before start:\n",
    "\n",
    "        sox를 설치하여야 아래 함수를 수행할 수 있다.\n",
    "\n",
    "        OS X: brew install sox\n",
    "\n",
    "        linux: apt-get install sox\n",
    "\n",
    "        windows: exe 파일을 다운받아 실행: https://sourceforge.net/projects/sox/files/sox/14.4.1/\n",
    "\n",
    "    Parameters:\n",
    "        file_path_list: 변환하고자 하는 wav 파일 리스트\n",
    "\n",
    "        output_path: 변환된 결과물을 저장하고자 하는 폴더 경로.\n",
    "\n",
    "        target_sample_rate: 변환하고자 하는 sample rate\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "    tfm = sox.Transformer()\n",
    "    tfm.convert(samplerate=target_sample_rate)\n",
    "\n",
    "    for i in tqdm(range(len(file_path_list))):\n",
    "        if not os.path.exists(file_path_list[i]):\n",
    "            raise OSError(f'File {file_path_list[i]} not exist')\n",
    "\n",
    "        file = file_path_list[i].rsplit('/', 1)[1]\n",
    "        output_file_path = os.path.join(output_path, file)\n",
    "\n",
    "        try:\n",
    "            tfm.build(file_path_list[i], output_file_path)\n",
    "        except:\n",
    "            print(f'Error in {file_path_list[i]}')\n",
    "            continue\n",
    "\n",
    "\n",
    "for state in origin_sound_files:\n",
    "    output_path = os.path.join(temp_data_path, state)\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    resampling(\n",
    "        file_path_list=origin_sound_files[state],\n",
    "        output_path=output_path,\n",
    "        target_sample_rate=16000\n",
    "    )\n",
    "\n",
    "temp_sound_files = get_sound_files_by_state(temp_data_path, state_list)\n",
    "for state in state_list:\n",
    "    print(\n",
    "        f'{state}: {len(temp_sound_files[state])}', temp_sound_files[state][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3번 과정의 함수를 정의한다.\n",
    "from scipy.signal import stft\n",
    "\n",
    "\n",
    "def read_wav(file_path):\n",
    "    with wave.open(file_path, 'r') as wav_file:\n",
    "        n_channels, sampwidth, framerate, n_frames, comptype, compname = wav_file.getparams()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "        audio_data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "    # Stereo 파일의 경우 mono로 변환\n",
    "    if n_channels == 2:\n",
    "        audio_data = (audio_data[::2] + audio_data[1::2]) / 2\n",
    "\n",
    "    return [audio_data, {'n_channels': n_channels,\n",
    "                         'sampwidth': sampwidth,\n",
    "                         'framerate': framerate,\n",
    "                         'n_frames': n_frames,\n",
    "                         'comptype': comptype,\n",
    "                         'compname': compname}]\n",
    "\n",
    "\n",
    "def mel_filter_bank(num_filters, fft_size, sample_rate):\n",
    "    \"\"\"\n",
    "    멜 필터뱅크 생성\n",
    "    \"\"\"\n",
    "    # 멜 스케일과 헤르츠 스케일 간의 변환 함수\n",
    "    def hz_to_mel(hz): return 2595 * np.log10(1 + hz / 700)\n",
    "    def mel_to_hz(mel): return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "    # 멜 스케일로 끝점 설정\n",
    "    mel_end = hz_to_mel(sample_rate / 2)\n",
    "    mel_points = np.linspace(hz_to_mel(0), mel_end, num_filters + 2)\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "\n",
    "    # FFT 주파수 인덱스로 변환\n",
    "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
    "\n",
    "    # 필터뱅크 생성\n",
    "    filters = np.zeros((num_filters, fft_size // 2 + 1))\n",
    "    for i in range(1, num_filters + 1):\n",
    "        filters[i - 1, bin_points[i - 1]:bin_points[i]] = \\\n",
    "            (np.arange(bin_points[i - 1], bin_points[i]) -\n",
    "             bin_points[i - 1]) / (bin_points[i] - bin_points[i - 1])\n",
    "        filters[i - 1, bin_points[i]:bin_points[i + 1]] = 1 - \\\n",
    "            (np.arange(bin_points[i], bin_points[i + 1]) -\n",
    "             bin_points[i]) / (bin_points[i + 1] - bin_points[i])\n",
    "    return filters\n",
    "\n",
    "\n",
    "def get_log_mel_spectogram(audio_data, framerate, num_filters=40, nperseg=2048, noverlap=1024, nfft=2048):\n",
    "    # STFT 계산\n",
    "    _, _, Zxx = stft(audio_data, fs=framerate, nperseg=nperseg,\n",
    "                     noverlap=noverlap, nfft=nfft)\n",
    "    magnitude = np.abs(Zxx)\n",
    "\n",
    "    # 로그 멜 스펙트럼 추출\n",
    "    num_filters = num_filters\n",
    "    filters = mel_filter_bank(num_filters, 2048, framerate)\n",
    "    mel_spectrum = np.dot(filters, magnitude)\n",
    "    log_mel_spectrum = np.log(mel_spectrum + 1e-9)  # log 0을 피하기 위한 작은 값 추가\n",
    "    return log_mel_spectrum\n",
    "\n",
    "\n",
    "def get_mel_power(log_mel_spectrum):\n",
    "    # 로그 멜 스펙트럼의 파워 계산\n",
    "    return np.sum(log_mel_spectrum**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4번 과정의 함수를 정의한다.\n",
    "def get_interval_min_sec(power, sec: int = 1):\n",
    "    interval_sec = int(10 * sec)\n",
    "    min_sec_list = []\n",
    "    for i in range(0, len(power) + 1, interval_sec):\n",
    "        argsort = np.argsort(power[i:i + interval_sec])\n",
    "        if len(argsort) < 1:\n",
    "            break\n",
    "        min_sec = i + argsort[0]\n",
    "        min_sec_list.append(min_sec)\n",
    "    return min_sec_list\n",
    "\n",
    "\n",
    "def split_audio_on_indices(audio_data, power, indices):\n",
    "    \"\"\"\n",
    "    주어진 인덱스를 기준으로 오디오 데이터를 여러 부분으로 나누는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_data: 오디오 데이터 배열\n",
    "    - indices: 분할할 시점의 리스트\n",
    "\n",
    "    Returns:\n",
    "    - audio_segments: 나눈 오디오 데이터의 리스트\n",
    "    \"\"\"\n",
    "    # STFT를 통해 구한 시점을 오디오 샘플의 시점으로 변환\n",
    "    nperseg = 2048\n",
    "    noverlap = 1024\n",
    "    samples_per_frame = (len(audio_data) - nperseg) / (len(power) - 1)\n",
    "    sample_indices = [int(index * samples_per_frame) for index in indices]\n",
    "\n",
    "    audio_segments = []\n",
    "    prev_index = 0\n",
    "    for index in sample_indices:\n",
    "        segment = audio_data[prev_index:index]\n",
    "        if len(segment) != 0:\n",
    "            audio_segments.append(segment)\n",
    "        prev_index = index\n",
    "    audio_segments.append(audio_data[prev_index:])  # 마지막 세그먼트 추가\n",
    "\n",
    "    return audio_segments\n",
    "\n",
    "\n",
    "def save_audio_segments_as_wav(audio_segments, output_dir, prefix, sampwidth, framerate):\n",
    "    \"\"\"\n",
    "    주어진 오디오 세그먼트들을 WAV 파일로 저장하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_segments: 나눈 오디오 데이터의 리스트\n",
    "    - output_dir: 출력 디렉토리 경로\n",
    "    - prefix: 저장할 파일의 접두사\n",
    "\n",
    "    Returns:\n",
    "    - filepaths: 저장된 파일의 경로 리스트\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    filepaths = []\n",
    "    for i, segment in enumerate(audio_segments):\n",
    "        filename = f\"{prefix}_{uuid4()}.wav\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with wave.open(filepath, 'wb') as wav_file:\n",
    "            wav_file.setnchannels(1)\n",
    "            wav_file.setsampwidth(sampwidth)\n",
    "            wav_file.setframerate(framerate)\n",
    "            wav_file.writeframes(segment.tobytes())\n",
    "        filepaths.append(filepath)\n",
    "\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaewone/ENTER/envs/tf24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# 5,6,7번 과정의 함수를 정의하거나 불러온다.\n",
    "from scipy.io import wavfile\n",
    "from typing import Optional\n",
    "import wave\n",
    "import numpy as np\n",
    "import noisereduce as nr\n",
    "\n",
    "from utils.os import *\n",
    "from utils.sound import *\n",
    "\n",
    "\n",
    "def reduced_base_noise(file_path: str,\n",
    "                       output_path: Optional[str] = None,\n",
    "                       inplace: bool = False):\n",
    "    \"\"\"\n",
    "    noisereduce 라이브러리를 이용하여 기본적인 노이즈를 감소시킨다.\n",
    "\n",
    "    Parameters:\n",
    "        * file_path : 처리하고자 하는 파일의 경로\n",
    "        * output_path : 처리한 결과를 저장하고자 하는 파일 경로. 없을 경우 file_path의 파일을 덮어쓴다.\n",
    "        * inplace : 원본 데이터(file_path)를 처리한 파일로 덮어쓴다.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    if inplace == False and output_path == None:\n",
    "        raise ValueError(f'output path must be defined if inplace is False.')\n",
    "\n",
    "    if output_path == None or inplace == True:\n",
    "        output_path = file_path\n",
    "\n",
    "    # load data\n",
    "    rate, data = wavfile.read(file_path)\n",
    "\n",
    "    # perform noise reduction\n",
    "    reduced_noise = nr.reduce_noise(y=data, sr=rate)\n",
    "    wavfile.write(output_path, rate, reduced_noise)\n",
    "\n",
    "\n",
    "def detect_non_silence(audio_data: np.ndarray, threshold: float, frame_size: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    오디오 신호에서 무음이 아닌 섹션의 시작과 끝을 감지한다.\n",
    "\n",
    "    Parameters:\n",
    "        - audio_data (numpy.ndarray): 묵음을 감지해야 하는 오디오 데이터.\n",
    "        - threshold (float): 오디오가 무음으로 간주되는 에너지 임계값.\n",
    "        - frame_size (int): 오디오 에너지의 이동 평균을 계산하기 위해 고려할 샘플 수.\n",
    "\n",
    "    Returns:\n",
    "        - start (int): 비침묵 섹션의 시작 샘플.\n",
    "        - end (int): non-silence 섹션의 엔딩 샘플.\n",
    "    \"\"\"\n",
    "\n",
    "    moving_avg = np.convolve(audio_data, np.ones(\n",
    "        (frame_size,)) / frame_size, mode='valid')\n",
    "    non_silence = np.where(moving_avg > threshold)[0]\n",
    "\n",
    "    start = non_silence[0]\n",
    "    # compensate for the 'valid' mode in convolution\n",
    "    end = non_silence[-1] + frame_size\n",
    "\n",
    "    return start, end\n",
    "\n",
    "\n",
    "def trim_audio(file_path: str, output_path: Optional[str] = None, inplace: bool = False, frame_size=5000):\n",
    "    \"\"\"\n",
    "    오디오의 앞뒤에 존재하는 화이트 노이즈를 제거한다.\n",
    "    화이트 노이즈는 음성의 전체 에너지의 하위 10%에 해당하는 시점으로 정의한다.\n",
    "\n",
    "    Parameters:\n",
    "        * file_path : 처리하고자 하는 파일의 경로\n",
    "        * output_path : 처리한 결과를 저장하고자 하는 파일 경로. 없을 경우 file_path의 파일을 덮어쓴다.\n",
    "        * inplace : 원본 데이터(file_path)를 처리한 파일로 덮어쓴다.\n",
    "\n",
    "    Returns: None\n",
    "    \"\"\"\n",
    "\n",
    "    if inplace == False and output_path == None:\n",
    "        raise ValueError(f'output path must be defined if inplace is False.')\n",
    "\n",
    "    # wav 파일을 읽어온다.\n",
    "    with wave.open(file_path, \"r\") as file:\n",
    "        params = file.getparams()\n",
    "        n_frames = params[3]\n",
    "        audio_data = file.readframes(n_frames)\n",
    "        wave_data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "    # 오디오 시그널을 통한 에너지 계산\n",
    "    energy = np.abs(wave_data)\n",
    "\n",
    "    # 백색 잡음을 분류하기 위한 임계값을 설정.\n",
    "    # 전체 에너지의 하위 10%에 10을 곱하여 임계값을 설정하였으나 추가적은 고민이 필요하다.\n",
    "    threshold = np.percentile(energy, 10) * 10\n",
    "\n",
    "    # Use a larger frame size to get a moving average of the audio energy\n",
    "    frame_size = frame_size\n",
    "    error_files = []\n",
    "    try:\n",
    "        start, end = detect_non_silence(energy, threshold, frame_size)\n",
    "        trimmed_wave_data = wave_data[start:end]\n",
    "    except:\n",
    "        trimmed_wave_data = wave_data\n",
    "        error_files.append(file_path)\n",
    "    # print(start, end)\n",
    "\n",
    "    # trim된 numpy array를 wav 파일로 저장한다.\n",
    "    if inplace or output_path == None:\n",
    "        remove_file(file_path)\n",
    "        output_path = file_path\n",
    "    with wave.open(output_path, \"w\") as out_file:\n",
    "        out_file.setparams(params)\n",
    "        out_file.writeframes(trimmed_wave_data.tobytes())\n",
    "\n",
    "    return error_files\n",
    "\n",
    "\n",
    "def normalize_and_save_audio(input_path, output_path=None, inplace=True):\n",
    "    \"\"\"\n",
    "    Load an audio file from the given input path, normalize it, and save the normalized audio to the given output path.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path: Path to the input audio file.\n",
    "    - output_path: Path to save the normalized audio file.\n",
    "    - inplace: 이미 존재하는 파일에 덮어쓴다.\n",
    "    \"\"\"\n",
    "    if inplace == False and output_path == None:\n",
    "        raise ValueError(f'output path must be defined if inplace is False.')\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "    # Normalize the audio data\n",
    "    y_normalized = y / np.max(np.abs(y))\n",
    "\n",
    "    # Convert float to int16 (since wavfile.write requires int values)\n",
    "    y_normalized_int16 = (y_normalized * 32767).astype(np.int16)\n",
    "\n",
    "    if inplace:\n",
    "        os.remove(input_path)\n",
    "        output_path = input_path\n",
    "\n",
    "    # Save the normalized audio data to the specified output path\n",
    "    wavfile.write(output_path, sr, y_normalized_int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:03<00:00, 19.68it/s]\n",
      " 26%|██▌       | 18/69 [00:00<00:01, 38.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dog12.wav Error: unknown format: 65534\n",
      "Error in dog13.wav Error: unknown format: 65534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 26/69 [00:00<00:01, 28.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in dog14.wav Error: unknown format: 65534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69/69 [00:02<00:00, 27.68it/s]\n",
      "100%|██████████| 168/168 [00:03<00:00, 42.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relax: 137 ['/Users/jaewone/Downloads/FurEmotion-AI/data/dog/relax/relax_acafd77b-1241-4c54-b89b-1b8766667cf8.wav', '/Users/jaewone/Downloads/FurEmotion-AI/data/dog/relax/relax_6d4884ad-3223-4e54-aac9-c48dcecf526a.wav']\n",
      "hostile: 140 ['/Users/jaewone/Downloads/FurEmotion-AI/data/dog/hostile/hostile_518e376a-54e8-4db7-a042-dfcc6bc20620.wav', '/Users/jaewone/Downloads/FurEmotion-AI/data/dog/hostile/hostile_889fdebb-566b-4381-99a1-a2af29b5fd21.wav']\n",
      "whining: 300 ['/Users/jaewone/Downloads/FurEmotion-AI/data/dog/whining/whining_60de4a67-0b80-4694-83f3-1f255575bb99.wav', '/Users/jaewone/Downloads/FurEmotion-AI/data/dog/whining/whining_a6ce5c62-0f32-4fb3-a664-227f493d3e70.wav']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 3 ~ 7 번 과정을 수행한다.\n",
    "for state in temp_sound_files:\n",
    "    file_list = temp_sound_files[state]\n",
    "    save_data_path = os.path.join(data_path, state)\n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        file_state = file_list[i].rsplit('/', 2)[1]\n",
    "\n",
    "        \"\"\"   3. 로그 멜 스펙트럼을 통해 음성의 파워를 측정한다.   \"\"\"\n",
    "\n",
    "        # 파일을 읽어온다.\n",
    "        try:\n",
    "            audio_data, audio_status = read_wav(file_list[i])\n",
    "        except Exception as e:\n",
    "            print(f'Error in {file_list[i].rsplit(\"/\", 1)[1]}', f'Error: {e}')\n",
    "            continue\n",
    "\n",
    "        # 로그 멜 스펙트럼을 추출한 뒤\n",
    "        log_mel_spectogram = get_log_mel_spectogram(\n",
    "            audio_data, audio_status['framerate'])\n",
    "\n",
    "        # 파워를 측정한다.\n",
    "        power = get_mel_power(log_mel_spectogram)\n",
    "\n",
    "        \"\"\"   4. 파워의 지역 최솟값을 기준으로 음성을 분할한다.   \"\"\"\n",
    "\n",
    "        # 파워의 지역 최솟값의 시점들을 구한다.\n",
    "        min_sec_list = get_interval_min_sec(power, sec=search_in_sec)\n",
    "\n",
    "        # 지역 최솟값을 기준으로 오디오(numpy)값을 분할한다.\n",
    "        audio_segments_sec = split_audio_on_indices(\n",
    "            audio_data, power, min_sec_list)\n",
    "\n",
    "        # 분할된 값을 data_path에 저장한다.\n",
    "        saved_filepaths = save_audio_segments_as_wav(\n",
    "            audio_segments=audio_segments_sec,\n",
    "            output_dir=save_data_path,\n",
    "            prefix=file_state,\n",
    "            sampwidth=audio_status['sampwidth'],\n",
    "            framerate=audio_status['framerate'],\n",
    "        )\n",
    "\n",
    "        # # 분할된 음성 중 search_in_sec의 절반보다 긴 음성들에 대해 5,6 과정을 진행한다.\n",
    "        total_error_files = []\n",
    "        for file_path in saved_filepaths:\n",
    "            # WAV 파일 로드\n",
    "            y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "            # 파일의 길이(초) 출력\n",
    "            origin_duration = librosa.get_duration(y=y, sr=sr)\n",
    "            if origin_duration > 1:\n",
    "                # 5. 분할된 음성의 앞뒤 화이트 노이즈(특정 임계값 이하의 에너지를 가지는 시점)을 제거한다.\n",
    "                error_files = trim_audio(\n",
    "                    file_path, inplace=True, frame_size=1000)\n",
    "                if (len(error_files) > 0):\n",
    "                    total_error_files += error_files\n",
    "\n",
    "                # 6. 분할된 음성이 노이즈를 제거하여 패턴을 보다 잘 드러나게 한다.\n",
    "                reduced_base_noise(file_path, inplace=True)\n",
    "\n",
    "                # 7. 음성 정규화를 수행한다.\n",
    "                normalize_and_save_audio(file_path, inplace=True)\n",
    "            # else:\n",
    "            #     os.remove(file_path)\n",
    "\n",
    "data_sound_files = get_sound_files_by_state(data_path, state_list)\n",
    "for state in state_list:\n",
    "    print(\n",
    "        f'{state}: {len(data_sound_files[state])}', data_sound_files[state][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음성 길이 규정\n",
    "\n",
    "위 과정을 통해 음성의 특징을 가지는 각각의 전처리된 세그먼트가 wav 파일 형태로 저장되었다.\n",
    "\n",
    "모델의 입력 벡터로 구성하기 위해서는 모든 음성의 길이가 동일해야 함으로 모든 세그먼트의 평균 길이를 구한 뒤 Padding과 Trimming 과정을 통해 평균 길이로 음성 길이를 통일하고자 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 세그먼트 길이 측정\n",
    "\n",
    "아래 과정을 통해 평균 세그먼트의 (음성)길이는 2.05 임을 알았다. 이에 음성을 반올림값인 2초로 통일하고자 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 2.0532673310225307\n",
      "사용할 음성의 길이: 2\n"
     ]
    }
   ],
   "source": [
    "data_sound_files = get_sound_files_by_state(data_path, state_list)\n",
    "file_list = [\n",
    "    file for state in data_sound_files for file in data_sound_files[state]]\n",
    "duration_list = []\n",
    "for file_path in file_list:\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    duration_list.append(duration)\n",
    "    # if duration < 1:\n",
    "    #     os.remove(file_path)\n",
    "    # else:\n",
    "    #     duration_list.append(duration)\n",
    "sum = np.array(duration_list).sum()\n",
    "avg = sum / len(duration_list)\n",
    "avg_duration = round(avg)\n",
    "print(f'Average: {avg}')\n",
    "print(f'사용할 음성의 길이: {avg_duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8번 과정을 수행한다.\n",
    "\n",
    "def pad_audio_to_length(input_path, target_duration, output_path=None, inplace=False):\n",
    "    \"\"\"\n",
    "    wav 파일을 target_duration 길이로 맞춘다. \n",
    "    음성의 중심을 기준으로 target_duration보다 짧을 경우 앞뒤에 Padding을 추가하며 \n",
    "    음성의 중심을 기준으로 target_duration보다 길 경우 앞뒤를 자른다.\n",
    "    \"\"\"\n",
    "\n",
    "    if inplace == False and output_path == None:\n",
    "        raise ValueError(f'output path must be defined if inplace is False.')\n",
    "\n",
    "    # WAV 파일 읽기\n",
    "    y, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "    # 현재 오디오 파일의 길이 확인\n",
    "    current_duration = len(y) / sr\n",
    "\n",
    "    # target_duration과 비교하여 패딩 또는 trimming 필요 여부 확인\n",
    "    total_samples = int(target_duration * sr)\n",
    "\n",
    "    if len(y) < total_samples:\n",
    "        # 패딩 필요\n",
    "        padding_samples = total_samples - len(y)\n",
    "        left_padding = padding_samples // 2\n",
    "        right_padding = padding_samples - left_padding\n",
    "        processed_audio = np.pad(\n",
    "            y, (left_padding, right_padding), mode='constant')\n",
    "    else:\n",
    "        # trimming 필요\n",
    "        excess_samples = len(y) - total_samples\n",
    "        left_trim = excess_samples // 2\n",
    "        right_trim = excess_samples - left_trim\n",
    "        processed_audio = y[left_trim:-right_trim]\n",
    "\n",
    "    if inplace:\n",
    "        os.remove(input_path)\n",
    "        output_path = input_path\n",
    "\n",
    "    # 변환된 데이터를 WAV 파일로 저장\n",
    "    sf.write(output_path, processed_audio, sr)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "data_sound_files = get_sound_files_by_state(data_path, state_list)\n",
    "file_list = [\n",
    "    file for state in data_sound_files for file in data_sound_files[state]]\n",
    "\n",
    "for file_path in file_list:\n",
    "    pad_audio_to_length(file_path, avg_duration, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State relax with file counts: 137\n",
      "State hostile with file counts: 140\n",
      "State whining with file counts: 300\n",
      "\n",
      "가장 적게 존재하는 State는 whining 이며 개수는 137 이다.\n"
     ]
    }
   ],
   "source": [
    "# 9번 과정을 수행한다 : 음성의 유사도를 측정하여 하위 10%의 데이터는 제외한다.\n",
    "\n",
    "min_state = ''\n",
    "min_count = 100000    # 초기값: 불가능한 아주 큰 수\n",
    "for state in os.listdir(data_path):\n",
    "    count = len(os.listdir(os.path.join(data_path, state)))\n",
    "    print(f'State {state} with file counts: {count}')\n",
    "    if count < min_count:\n",
    "        min_count = count\n",
    "        min_state = state\n",
    "\n",
    "print(f'\\n가장 적게 존재하는 State는 {state} 이며 개수는 {min_count} 이다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state file length:  137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 137/137 [00:03<00:00, 38.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state file length:  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 140/140 [00:03<00:00, 39.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state file length:  300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 300/300 [00:16<00:00, 18.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best similar files: \n",
      "relax(14): relax_fb938076-df2c-4cec-8041-6fe70e69b619.wav, relax_ca4ac321-5dc9-482e-a0cb-c1685a644213.wav, relax_af40514e-adcb-4fd2-bd84-04b54b5e92d9.wav, relax_0a977d15-b25e-4e73-beb1-c46f361be830.wav, relax_edd2a521-dabc-4486-93f7-061ac491e31f.wav, relax_ad1f89ef-fe50-4788-b232-01435c05d66b.wav, relax_cfcb0d25-3ea6-465f-9862-e8f06ae8a638.wav, relax_f20900a0-7dc1-4720-969d-eec0f15b4cc5.wav, relax_bff9ff3d-c0a2-44aa-b424-80ac795e6217.wav, relax_1a0fa57a-3154-432b-b0d3-07632364979a.wav, relax_c7a34bf6-da89-4b12-93f1-56e0683d046e.wav, relax_446b3d4a-e1d6-4be2-b657-dec43d522bdb.wav, relax_a11c66b0-2aa5-4f3d-a026-32d8a0f41248.wav, relax_b2227d53-88e1-4ca8-8177-7871f8bd0968.wav, \n",
      "hostile(14): hostile_60815d15-6f45-45b5-b7f7-18604b3104c9.wav, hostile_771bff27-6c81-43fd-8ab2-fca8707c31c9.wav, hostile_da5d65c6-646d-4a4a-b47e-eb2de883400e.wav, hostile_6ba041c4-ee90-4fe3-bfc7-bd62818a41bb.wav, hostile_be723a29-f78f-4889-a305-9f64a42b1f5d.wav, hostile_ffa87c62-ab7f-4546-ba66-758d4766265e.wav, hostile_fa26e6ef-02dc-4f3a-9485-8ef3bb7e1fc7.wav, hostile_85ac96f5-8a8c-456a-9cf6-002662c72471.wav, hostile_1d181d37-c819-429b-a20f-80ebbd28d15a.wav, hostile_0a460ee9-c1a6-466c-ab7f-0ea9167f80f6.wav, hostile_ceacfdc2-f904-4d9d-9210-9f0934d1ec39.wav, hostile_5c5422ad-0406-4e53-9d70-9bc6dd190fad.wav, hostile_efe39e1c-d55c-483c-b93f-47ecd30f2aad.wav, hostile_52d08a13-d2c6-4ece-b2d6-1317b1a6022b.wav, \n",
      "whining(30): whining_9967a847-ccdb-4e93-ab20-165be3b0ee19.wav, whining_e482d45a-68ae-40cc-b5d4-78096bda5821.wav, whining_ec0068bc-3c05-440a-99b8-eb243e85ab2e.wav, whining_b7695955-f7b9-4b8c-9c0c-f19f5bd216a2.wav, whining_07604e19-99a6-4eef-8610-1a1c351396ed.wav, whining_54d9b8a4-69f7-4223-bc15-9ba5bf09ead5.wav, whining_aa12f97f-a7d1-4573-8f69-4e03c041d1ee.wav, whining_4de2d47d-ad60-418d-ac6b-2252a2cfbdae.wav, whining_f4161a9d-8036-41b7-b08f-5a4bc315b43b.wav, whining_9864a145-e129-4fc1-afe6-4fbab0f80fc8.wav, whining_58570b6a-3d9c-4b37-bb47-6b0f490e0285.wav, whining_d766b07a-9226-452e-8fbf-147aef998017.wav, whining_c1951d9e-f2db-4568-b057-d9d84d8f3e53.wav, whining_22d59a2b-2cb9-43be-bb55-27924e8be08d.wav, whining_d1d0b95e-aa22-4914-99b7-02ddb3c2da36.wav, whining_bf1a430d-1e13-4012-b41c-48b3953d4ebe.wav, whining_6bfcb1cf-979e-4421-9d1d-63a0566cda70.wav, whining_c49d5db1-7b82-4b1b-bf00-42e489ede7b6.wav, whining_0949a194-b451-4f7b-a835-486aab4a76f2.wav, whining_800f8f99-cbfd-4c58-8dbd-02710158a09b.wav, whining_1ad486c1-6a6b-4508-9c82-7d84f67cb995.wav, whining_a2fec04c-c45e-40dc-be13-4e0f53e91b03.wav, whining_c226124e-0673-4cab-84b6-509c1f10488b.wav, whining_fa121e4a-3b4d-44f6-8c61-e9393839cb96.wav, whining_7ed06b06-1a6d-4388-a964-d3cc11954278.wav, whining_ed0730c2-ea82-4992-9227-0d989dece00c.wav, whining_ff9c15d5-6d9b-4e4b-bf35-72a102ad7665.wav, whining_5f2ffb07-6593-4c59-bb66-f150c69b2047.wav, whining_38fc3e3b-2eb6-489e-83bf-9a45a56a62c8.wav, whining_818cf83d-2a9e-47b0-8acc-c349a62d41a6.wav, \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 9번 과정을 수행한다 : 유사도를 측정한 뒤 전체 파일의 개수를 맞춘다.\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import Optional\n",
    "from tqdm import tqdm\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "from utils.os import remove_file\n",
    "\n",
    "\n",
    "def compute_melspectrogram(file_path: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    wav 파일의 멜 스펙토그램을 계산한다.\n",
    "\n",
    "    Parameters:\n",
    "        * file_path : wav 파일의 경로\n",
    "\n",
    "    Returns:\n",
    "        * 계산된 멜 스펙트럼값(numpy array)\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path)\n",
    "    mels = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    return mels\n",
    "\n",
    "\n",
    "def get_similarities(file_list: list[str], top_n: Optional[int] = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    파일 리스트에서 가장 유사한 n개의 파일을 반환한다.\n",
    "\n",
    "    Parameters:\n",
    "    * file_list: 분석하고자 하는 파일 경로 리스트\n",
    "    * top_n: 유사한 순으로 나열했을 때 반환 할 상위 n개의 파일 리스트. 만약 top_n=None 일 경우 전체 리스트를 반환한다.\n",
    "\n",
    "    Returns: 유사도 순으로 나열한 n개의 파일 리스트\n",
    "    \"\"\"\n",
    "\n",
    "    # 멜 스펙트럼을 분석한다.\n",
    "    melspectrograms = {file_path: compute_melspectrogram(\n",
    "        file_path) for file_path in file_list}\n",
    "\n",
    "    # 코사인 유사도를 측정한다.\n",
    "    n = len(file_list)\n",
    "    similarity_matrix = np.zeros((n, n))\n",
    "    with tqdm(total=n, desc='Processing', position=0) as pbar:\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                mels1 = melspectrograms[file_list[i]].flatten()\n",
    "                mels2 = melspectrograms[file_list[j]].flatten()\n",
    "\n",
    "                # 두 멜 스펙트럼의 크기가 다를 경우 작은 크기에 맞춘다.\n",
    "                min_size = min(mels1.shape[0], mels2.shape[0])\n",
    "                mels1 = mels1[:min_size]\n",
    "                mels2 = mels2[:min_size]\n",
    "\n",
    "                similarity_matrix[i, j] = cosine_similarity(\n",
    "                    mels1.reshape(1, -1), mels2.reshape(1, -1))\n",
    "            pbar.update(1)\n",
    "\n",
    "    # 유사도 순으로 나열한다.\n",
    "    np.fill_diagonal(similarity_matrix, 0)\n",
    "    indices = np.flip(np.argsort(np.sum(similarity_matrix, axis=1)))\n",
    "\n",
    "    return [file_list[i] for i in (indices if top_n == None else indices[:top_n])]\n",
    "\n",
    "\n",
    "def delete_dissimilar_wavs(data_path: str, n_limit: int, save_dir=None):\n",
    "    mask_files = {}\n",
    "    for state in state_list:\n",
    "        mask_files[state] = []\n",
    "        state_path = os.path.join(data_path, state)\n",
    "        state_file_list = [os.path.join(state_path, file)\n",
    "                           for file in os.listdir(state_path) if file.endswith('.wav')]\n",
    "        print(\"state file length: \", len(state_file_list))\n",
    "        mask_file_length = round(len(state_file_list) * 0.1)\n",
    "\n",
    "        # get similarities\n",
    "        sim_file_list = get_similarities(state_file_list)\n",
    "\n",
    "        # 파일의 개수를 n_limit으로 맞춘다.\n",
    "        del_file_list = sim_file_list[n_limit:]\n",
    "        for file in del_file_list:\n",
    "            remove_file(file)\n",
    "\n",
    "        # 유사도 정도를 저장한다.\n",
    "        sim_file_list = sim_file_list[:n_limit]\n",
    "        if (save_dir != None):\n",
    "            np.savetxt(f'{os.path.join(save_dir, state)}_similarity.txt',\n",
    "                       sim_file_list, delimiter=',', fmt='%s')\n",
    "\n",
    "        mask_files[state] = sim_file_list[:mask_file_length]\n",
    "    return mask_files\n",
    "\n",
    "\n",
    "save_dir = os.path.join(main_path, 'save_similarity')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "mask_files = delete_dissimilar_wavs(data_path, min_count, save_dir)\n",
    "print(\"\\nBest similar files: \")\n",
    "for state in mask_files:\n",
    "    print(f'{state}({len(mask_files[state])}):', end=' ')\n",
    "    for file in mask_files[state]:\n",
    "        print(file.rsplit('/', 1)[1], end=', ')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:03<00:00,  4.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State relax masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [00:02<00:00,  4.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State hostile masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:06<00:00,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State whining masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 10번 과정을 수행한다.\n",
    "# 유사도 상위 10% 데이터에 대하여 SpecAugment를 적용하여 Training 데이터에 대해 마스킹 작업을 수행한다.\n",
    "\n",
    "def mask_melspectrogram(file_path,\n",
    "                        freq_mask_num=2,\n",
    "                        time_mask_num=2,\n",
    "                        freq_masking_max_percentage=0.15,\n",
    "                        time_masking_max_percentage=0.3):\n",
    "\n",
    "    # 1. Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # 2. Extract mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "    num_freq_bins = mel_spectrogram.shape[0]\n",
    "    num_time_bins = mel_spectrogram.shape[1]\n",
    "\n",
    "    # 3. Apply masking\n",
    "    # Frequency masking\n",
    "    for _ in range(freq_mask_num):\n",
    "        freq_start = np.random.randint(0, num_freq_bins)\n",
    "        freq_length = np.random.randint(\n",
    "            0, int(num_freq_bins * freq_masking_max_percentage))\n",
    "        freq_end = min(mel_spectrogram.shape[0], freq_start + freq_length)\n",
    "        mel_spectrogram[freq_start:freq_end, :] = 0\n",
    "\n",
    "    # Time masking\n",
    "    for _ in range(time_mask_num):\n",
    "        time_start = np.random.randint(0, num_time_bins)\n",
    "        time_length = np.random.randint(\n",
    "            0, int(num_time_bins * time_masking_max_percentage))\n",
    "        time_end = min(mel_spectrogram.shape[1], time_start + time_length)\n",
    "        mel_spectrogram[:, time_start:time_end] = 0\n",
    "\n",
    "    return mel_spectrogram\n",
    "\n",
    "\n",
    "def show_masked_mel(db_masked_mel):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(db_masked_mel, origin='lower', aspect='auto',\n",
    "               extent=[0, db_masked_mel.shape[1], 0, db_masked_mel.shape[0]])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Realistically Masked Mel spectrogram (using matplotlib)')\n",
    "    plt.xlabel('Time frames')\n",
    "    plt.ylabel('Mel frequency bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_masked_spectrogram_to_wav(mel_spectrogram, sr, save_path):\n",
    "    \"\"\" Convert masked mel spectrogram back to waveform and save as wav \"\"\"\n",
    "    y_inv = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr)\n",
    "    sf.write(save_path, y_inv, sr)\n",
    "\n",
    "\n",
    "def create_mask(wav_file, output_dir=None, n_create=2):\n",
    "    sr = 16000\n",
    "    wav_output_dir, wav_file_name = wav_file.rsplit('/', 1)\n",
    "    base_filename = wav_file_name.split('.')[0]\n",
    "\n",
    "    if output_dir == None:\n",
    "        output_dir = wav_output_dir\n",
    "\n",
    "    for i in range(1, n_create + 1, 1):\n",
    "        odd = i % 2 == 0\n",
    "        masked_mel = mask_melspectrogram(\n",
    "            wav_file,\n",
    "            freq_mask_num=1,\n",
    "            time_mask_num=1,\n",
    "            freq_masking_max_percentage=0.12 if odd else 0.07,\n",
    "            time_masking_max_percentage=0.07 if odd else 0.12\n",
    "        )\n",
    "        save_masked_spectrogram_to_wav(masked_mel, sr, os.path.join(\n",
    "            output_dir, base_filename + f\"_mask{i}.wav\"))\n",
    "        # show_masked_mel(librosa.power_to_db(masked_mel, ref=np.max))\n",
    "        # print(f'Save: {os.path.join(output_dir, base_filename + f\"_mask{i}.wav\")}')\n",
    "\n",
    "\n",
    "top_10 = int(len(os.listdir(os.path.join(data_path, state_list[0]))) * 0.1)\n",
    "\n",
    "for state in state_list:\n",
    "    state_data_path = os.path.join(data_path, state)\n",
    "    file_list = mask_files[state]\n",
    "    for i in tqdm(range(len(file_list))):\n",
    "        create_mask(\n",
    "            wav_file=file_list[i],\n",
    "            output_dir=state_data_path,\n",
    "            n_create=1\n",
    "        )\n",
    "    print(f'State {state} masking done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파일 이름을 relax_1.wav, relax_2.wav, ... 형태로 변경하여 보기 쉽게 한다.\n",
    "def rename_files(prefix: str, dir_path: str):\n",
    "    file_list = [file for file in os.listdir(\n",
    "        dir_path) if file.endswith('.wav')]\n",
    "    for i in range(len(file_list)):\n",
    "        os.rename(os.path.join(dir_path, file_list[i]),\n",
    "                  os.path.join(dir_path, f'{prefix}_{i+1}.wav'))\n",
    "\n",
    "\n",
    "for state in data_sound_files:\n",
    "    rename_files(state, os.path.join(data_path, state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_dir를 삭제한다.\n",
    "from utils.os import remove_path_with_files\n",
    "\n",
    "remove_path_with_files(temp_data_path)\n",
    "print('All process done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
